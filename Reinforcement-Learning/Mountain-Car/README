RL - Model-Free - Q-Learning - Quantile Regression + Deep Q-Learning (QR-DQN)

In reinforcement learning, the value of an action a in state s describes the expected return obtained from beginning in that state, choosing action a, then following a 
prescribed policy. Knowing the value for the optimal policy is sufficient to act optimally, it is hence the object modelled by classic value-based methods such as SARSA 
or Q-Learning, which use Bellman’s equation, to calculate that value -- Bellman's equation is associated with dynamic programming, it writes the value of a decision problem 
at a certain point in time in terms of the payoff from some initial choices and the value of the remaining decision problem that results from those initial choices.

As an agent interacts with the environment by taking actions and observing the next state and reward, when sampled probabilistically, these state transitions, rewards, and 
actions can all induce randomness in the observed long-term return. Traditional RL algorithms average over this randomness to estimate the value function.

In 2017, it was shown that the distribution of the random returns, whose expectation constitutes that value, can be described by the distributional Bellman operator (i.e., 
operator which describes the expected behaviour of popular learning algorithms such as SARSA and Q-Learning). This operator is an extreme (maximal) form of the Wasserstein 
metric (i.e., metric on probability distributions that measures the minimal effort required to reconfigure the probability mass of one distribution in order to recover another
one) -- the issue is that the Wasserstein metric, viewed as a loss function, cannot be easily minimized using stochastic gradient methods.

This leaves a theory-practice gap in our understanding of distributional RL, as makes it difficult to explain the good performance of a model. The objective of this section is
to describe a distributional algorithm that operates end-to-end on the Wasserstein metric, and that resolves tha above issue -- i.e., this algorithm, applicable in a stochastic 
approximation setting, can perform distributional RL over the Wasserstein metric.

The particularity of this model is that distribution over returns is modeled explicitly instead of only by estimating the mean -- i.e., we examine here methods of learning the 
value distribution instead of the value function, like done in section xx of this website.

Without going into too many details, the data scientist who worked on this algorithm followed the below approach:
i. extend an existing classic model that uses distributional approximation instead of traditional practican non-explanatory approximation
ii. show in this distributional RL algo that quantile regression may be used to stochastically adjust the distributions’ locations to minimize the Wasserstein distance to a 
target distribution.
iii. evaluate the new algo on other games (i.e., Atari 2600 games), observing that it significantly outperforms the previous algos (i.e., gained 33% median score increment)

Important points to keep in mind:
i. in distributional RL, the distribution over returns (i.e., value distribution) plays the central role and replaces the traditionally used value function 
ii. the value distribution is not designed to capture the uncertainty in the estimate of the value function, but rather the randomness in the returns of Markov-Decision-Process
(i.e., decision making framework for situations where outcomes are partly random and partly under the control of a decision maker)
iii. value distribution is computed through dynamic programming using a distributional Bellman operator
iv. the Wasserstein metric is a probability metric that considers both the probability of and the distance between various outcome events. This makes it well-suited to domains 
where a similarity in outcome is more important than exactly matching likelihoods.
v. the key factor that allowed the minimization of Wasserstein metric using stochastic gradient was to consider fixed probabilities but variable locations when parametrizing 
the calculation function
vi. this new approximation aims to estimate quantiles of the target distribution, which is why it is called a quantile distribution -- a quantile distribution maps each 
state-action pair to a uniform probability distribution
vi. quantile regression's loss function penalizes over-estimation errors with a weight 't', and underestimation errors with weight '1−t'. For a distribution and a given 
quantile 't', the value of the quantile function = minimizer of the quantile regression loss -- by Lemma2 we can find minimizing values of errors by stochastic gradient descent
vii. this model approximates the value distribution with a parameterized quantile distribution over the set of quantile midpoints, defined by Lemma 2, then, trains the 
location parameters using quantile regression
viii. the algorithm here is built on the DQN architecture, but it requires a modification to DQN: it changes the size of the NN output layer to |A| × N, where N is a 
hyper-parameter giving the number of quantile targets

Compared to the original parametrization, the benefits of a parameterized quantile distribution are:
i. no restriction to prespecified bounds on the support, potentially leading to significantly more accurate predictions when the range of returns vary greatly across states 
-- support of a probability distribution = closure of the set of possible values of a random variable having that distribution. More specifically, QR-DQN does not require 
projection onto the approximating distribution’s support as it is able to expand or contract the values arbitrarily to cover the true range of return values, which means that 
QR-DQN does not require the additional hyper-parameter giving the bounds of the support. The only additional hyper-parameter of QR-DQN is the number of quantiles N, which 
controls with what resolution we approximate the value distribution (As we increase N, QR-DQN goes from DQN to increasingly able to estimate the upper and lower quantiles of 
the value distribution; it becomes increasingly capable of distinguishing low probability events at either end of the cumulative distribution over returns)
ii. it eliminates issues of disjoint supports, as by using this approach, we no longer need to have the domain knowledge about the bounds of the return distribution when 
applying the algorithm to new tasks.
iii. it allows us to minimize the Wasserstein loss, without suffering from biased gradients

To conclude, learning the distribution over returns has distinct advantages over learning the value function alone. Distributional reinforcement learning learn the true 
distribution over returns, show increased robustness during training, and significantly improve sample complexity and final performance over baseline algorithms.
